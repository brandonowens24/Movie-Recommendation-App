{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', None)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_movies = pd.read_csv(\"TMDB_movie_dataset_v11.csv\")\n",
    "movies = pd.read_json(\"movies.json\")\n",
    "\n",
    "tmdb_shows = pd.read_csv(\"TMDB_tv_dataset_v3.csv\")\n",
    "shows = pd.read_json(\"shows.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.dropna(subset=[\"Title\", \"Plot\", \"Genre\", \"Type\", \"Director\"])\n",
    "movies = movies[(movies[\"Plot\"] != \"N/A\") & (movies[\"Plot\"] != \"nan\") & (movies[\"Title\"] != \"N/A\") & (movies[\"Type\"] != \"series\") & (movies[\"Director\"] != \"N/A\")]\n",
    "movies = movies.drop_duplicates([\"Title\"], keep='first')\n",
    "\n",
    "shows = shows.dropna(subset=[\"Title\", \"Plot\", \"Genre\", \"Type\"])\n",
    "shows = shows[(shows[\"Plot\"] != \"N/A\") & (shows[\"Plot\"] != \"nan\") & (shows[\"Title\"] != \"N/A\") & (shows[\"Type\"] != \"movie\")]\n",
    "shows = shows.drop_duplicates(subset=[\"Title\"], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.merge(movies, tmdb_movies, how=\"left\", left_on=\"imdbID\", right_on=\"imdb_id\")\n",
    "shows = pd.merge(shows, tmdb_shows, how=\"left\", left_on=\"Title\", right_on=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows = shows[shows[\"origin_country\"].str.contains(\"US\") | shows[\"origin_country\"].str.contains(\"JP\")]\n",
    "movies = movies[movies[\"original_language\"]==\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[\"description\"] = movies.apply(lambda x: f\"A {x['Genre']} {x['Type']} directed by {x['Director']} with keywords consisting of {x['keywords']}. {x['Plot']}\", axis=1)\n",
    "shows[\"description\"] = shows.apply(lambda x: f\"A {x['Genre']} {x['Type']}. {x['Plot']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194482\n",
      "17751\n"
     ]
    }
   ],
   "source": [
    "print(movies.shape[0])\n",
    "print(shows.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Corpora Tokenized (Movies and Shows Separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    '''\n",
    "    Takes a document and returns a list of tokens from all the sentences in that document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document : str\n",
    "        The body of text you would like to tokenize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_tokens : list\n",
    "        A list of tokens comprising the sentences in that document.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    document = \"A Crime, Drama, Thriller movie directed by Ethan Coen. It was a super good film.\"\n",
    "\n",
    "    doc_tokens = tokenize(document)\n",
    "\n",
    "    Will output:\n",
    "        \"['a', 'crime', 'drama', 'thriller', 'movie', 'directed', 'by' 'ethan', 'coen', 'it', 'was', 'a', 'super', 'good', 'film']\"\n",
    "    '''\n",
    "    doc_tokens = []\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        sent_tokens = nltk.word_tokenize(sentence)\n",
    "        sent_tokens = [word.lower() for word in sent_tokens if word]\n",
    "        doc_tokens += sent_tokens\n",
    "    return doc_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194482/194482 [01:38<00:00, 1978.19it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sentences_tokenized_movies = []\n",
    "\n",
    "documents = list(movies.description)\n",
    "\n",
    "for document in tqdm(documents):\n",
    "    doc_tokens = tokenize(document)\n",
    "    all_sentences_tokenized_movies += [doc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17751 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17751/17751 [00:07<00:00, 2466.59it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sentences_tokenized_shows = []\n",
    "\n",
    "documents = list(shows.description)\n",
    "\n",
    "for document in tqdm(documents):\n",
    "    doc_tokens = tokenize(document)\n",
    "    all_sentences_tokenized_shows += [doc_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Separate Counts Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = all_sentences_tokenized_movies\n",
    "vector_size = 300\n",
    "window = 5\n",
    "workers = 6\n",
    "hs = 1\n",
    "sg = 0\n",
    "seed = 27\n",
    "min_count = 1\n",
    "epochs = 250\n",
    "compute_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    workers=workers,\n",
    "    hs=hs,\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    min_count=min_count,\n",
    "    compute_loss=compute_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cinema-rec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = all_sentences_tokenized_shows\n",
    "vector_size = 300\n",
    "window = 5\n",
    "workers = 6\n",
    "hs = 1\n",
    "sg = 0\n",
    "seed = 27\n",
    "min_count = 1\n",
    "epochs = 250\n",
    "compute_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    workers=workers,\n",
    "    hs=hs,\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    min_count=min_count,\n",
    "    compute_loss=compute_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"show-rec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"cinema-rec.model\")\n",
    "model2 = Word2Vec.load(\"show-rec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_join(document):\n",
    "    '''\n",
    "    Takes a document and returns a list of tokens from all the sentences in that document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document : str\n",
    "        The body of text you would like to tokenize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_tokens : list\n",
    "        A list of tokens comprising the sentences in that document.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    document = \"A Crime, Drama, Thriller movie directed by Ethan Coen. It was a super good film.\"\n",
    "\n",
    "    doc_tokens = tokenize(document)\n",
    "\n",
    "    Will output:\n",
    "        \"['a', 'crime', 'drama', 'thriller', 'movie', 'directed', 'by' 'ethan', 'coen', 'it', 'was', 'a', 'super', 'good', 'film']\"\n",
    "    '''\n",
    "    doc_tokens = []\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        sent_tokens = nltk.word_tokenize(sentence)\n",
    "        sent_tokens = [word.lower() for word in sent_tokens if word]\n",
    "        doc_tokens += sent_tokens\n",
    "    return \" \".join(doc_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194482/194482 [01:38<00:00, 1974.56it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sentences_tokenized_joined_movies = []\n",
    "\n",
    "for document in tqdm(list(movies.description)):\n",
    "    doc_tokens = tokenize_join(document)\n",
    "    all_sentences_tokenized_joined_movies += [doc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_tokenized_joined_shows = []\n",
    "\n",
    "for document in tqdm(list(shows.description)):\n",
    "    doc_tokens = tokenize_join(document)\n",
    "    all_sentences_tokenized_joined_shows += [doc_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = m_vectorizer.fit_transform(movies[\"description\"])\n",
    "\n",
    "movies_tfidf_features = m_vectorizer.get_feature_names_out()\n",
    "movies_tfidf_list = dict(zip(m_vectorizer.get_feature_names_out(), m_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194482/194482 [01:40<00:00, 1927.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize movie_tfidf_vectors\n",
    "movie_tfidf_vectors = np.zeros((len(movies), 300))\n",
    "\n",
    "for i, description in tqdm(enumerate(all_sentences_tokenized_joined_movies), total=len(all_sentences_tokenized_joined_movies)):\n",
    "    words = description.split()\n",
    "    tfidf_indices = [m_vectorizer.vocabulary_[word] for word in words if word in m_vectorizer.vocabulary_]\n",
    "    tfidf_values = [movies_tfidf_list[word] for word in words if word in m_vectorizer.vocabulary_]\n",
    "\n",
    "    for j, idx in enumerate(tfidf_indices):\n",
    "        movie_tfidf_vectors[i] += model.wv[words[j]] * tfidf_values[j]\n",
    "\n",
    "    movie_tfidf_vectors[i] /= np.sum(tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = s_vectorizer.fit_transform(shows[\"description\"])\n",
    "\n",
    "shows_tfidf_features = s_vectorizer.get_feature_names_out()\n",
    "shows_tfidf_list = dict(zip(s_vectorizer.get_feature_names_out(), s_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17751/17751 [00:06<00:00, 2850.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize movie_tfidf_vectors\n",
    "shows_tfidf_vectors = np.zeros((len(shows), 300))\n",
    "\n",
    "for i, description in tqdm(enumerate(all_sentences_tokenized_joined_shows), total=len(all_sentences_tokenized_joined_shows)):\n",
    "    words = description.split()\n",
    "    tfidf_indices = [s_vectorizer.vocabulary_[word] for word in words if word in s_vectorizer.vocabulary_]\n",
    "    tfidf_values = [shows_tfidf_list[word] for word in words if word in s_vectorizer.vocabulary_]\n",
    "\n",
    "    for j, idx in enumerate(tfidf_indices):\n",
    "        shows_tfidf_vectors[i] += model2.wv[words[j]] * tfidf_values[j]\n",
    "\n",
    "    shows_tfidf_vectors[i] /= np.sum(tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[\"tfidf_embedding\"] = list(movie_tfidf_vectors)\n",
    "shows[\"tfidf_embedding\"] = list(shows_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_counts_vectors = np.zeros((len(all_sentences_tokenized_joined_movies), 300))\n",
    "\n",
    "for i, description in enumerate(tqdm(all_sentences_tokenized_joined_movies)):  \n",
    "    words = description.split()\n",
    "    desc_vec = np.zeros((1, 300))\n",
    "    for word in words:\n",
    "        desc_vec += model2.wv[word] \n",
    "    desc_vec /= len(words)\n",
    "\n",
    "    movie_counts_vectors[i] = desc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17751/17751 [00:03<00:00, 4919.07it/s]\n"
     ]
    }
   ],
   "source": [
    "shows_counts_vectors = np.zeros((len(all_sentences_tokenized_joined_shows), 300))\n",
    "\n",
    "for i, description in enumerate(tqdm(all_sentences_tokenized_joined_shows)):  \n",
    "    words = description.split()\n",
    "    desc_vec = np.zeros((1, 300))\n",
    "    for word in words:\n",
    "        desc_vec += model2.wv[word] \n",
    "    desc_vec /= len(words)\n",
    "\n",
    "    shows_counts_vectors[i] = desc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[\"count_embedding\"] = list(movie_counts_vectors)\n",
    "shows[\"count_embedding\"] = list(shows_counts_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.to_json(\"movies_we.json\")\n",
    "shows.to_json(\"shows_we.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Separate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_sentences_tokenized_movies)]\n",
    "show_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_sentences_tokenized_shows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(movie_documents, vector_size=200, window=5, workers = 6, epochs = 150, hs = 1)\n",
    "model.save(\"doc2vec-cinema.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Doc2Vec(show_documents, vector_size=200, window=5, workers = 6, epochs = 150, hs = 1)\n",
    "model2.save(\"doc2vec-show.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Doc2Vec.load(\"doc2vec-cinema.model\")\n",
    "model2= Doc2Vec.load(\"doc2vec-show.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.loc[:, [\"Title\", \"Year\", \"Genre\", \"Director\", \"Plot\", \"description\", \"Poster\", \"Ratings\", \"Language\", \"Type\", \"count_embedding\"]]\n",
    "shows = shows.loc[:, [\"Title\", \"Year\", \"Genre\", \"Plot\", \"description\", \"Poster\", \"Ratings\", \"Language\", \"Type\", \"count_embedding\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.reset_index(inplace=True, drop=True)\n",
    "shows.reset_index(inplace=True, drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
